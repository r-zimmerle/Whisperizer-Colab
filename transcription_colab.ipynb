{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2142,
     "status": "ok",
     "timestamp": 1740960109772,
     "user": {
      "displayName": "NEXUS - DCG",
      "userId": "08238528941623053962"
     },
     "user_tz": 180
    },
    "id": "R1LgceSIRlB5",
    "outputId": "986a9e7c-d2f9-456b-fd2e-3a3f6c4a4af2"
   },
   "outputs": [],
   "source": [
    "# 1) Mount Google Drive\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 15,
     "status": "ok",
     "timestamp": 1740960113640,
     "user": {
      "displayName": "NEXUS - DCG",
      "userId": "08238528941623053962"
     },
     "user_tz": 180
    },
    "id": "3_g2EL-oSdOr",
    "outputId": "613dd686-73ca-42c6-95fd-903bdd98e301"
   },
   "outputs": [],
   "source": [
    "# 2) Install dependencies (FFmpeg and Whisper)\n",
    "!sudo apt-get update -qq\n",
    "!sudo apt-get install -y ffmpeg\n",
    "!pip install -q git+https://github.com/openai/whisper.git\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1102,
     "status": "ok",
     "timestamp": 1740960116721,
     "user": {
      "displayName": "NEXUS - DCG",
      "userId": "08238528941623053962"
     },
     "user_tz": 180
    },
    "id": "yFLcR332Sfxy",
    "outputId": "ccac9032-365b-4b63-ba06-d36869b3bd3e"
   },
   "outputs": [],
   "source": [
    "# 3) Imports and GPU check for Whisper\n",
    "import os\n",
    "import subprocess\n",
    "import torch\n",
    "import whisper\n",
    "\n",
    "# Check if a GPU is available for Whisper\n",
    "gpu_available = torch.cuda.is_available()\n",
    "gpu_name = torch.cuda.get_device_name(0) if gpu_available else \"No GPU detected\"\n",
    "device = \"cuda\" if gpu_available else \"cpu\"\n",
    "\n",
    "print(f\"GPU available for Whisper: {gpu_available}\")\n",
    "print(f\"GPU name: {gpu_name}\")\n",
    "print(f\"Whisper device: {device}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 8,
     "status": "ok",
     "timestamp": 1740960119222,
     "user": {
      "displayName": "NEXUS - DCG",
      "userId": "08238528941623053962"
     },
     "user_tz": 180
    },
    "id": "6GA1x9_7Sune",
    "outputId": "d0cdb00b-0adb-41a4-d754-af17c8a5e351"
   },
   "outputs": [],
   "source": [
    "def convert_videos_to_audio(input_folder, output_folder):\n",
    "    \"\"\"\n",
    "    Convert video files in the supported formats to M4A audio using FFmpeg (CPU).\n",
    "    We keep it on CPU because GPU acceleration for audio extraction is typically\n",
    "    not a big performance boost, and can complicate dependencies.\n",
    "    \"\"\"\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "    supported_formats = (\".mp4\", \".mov\", \".avi\", \".mkv\")\n",
    "\n",
    "    for file in os.listdir(input_folder):\n",
    "        if file.lower().endswith(supported_formats):\n",
    "            input_path = os.path.join(input_folder, file)\n",
    "            output_name = os.path.splitext(file)[0] + \".m4a\"\n",
    "            output_path = os.path.join(output_folder, output_name)\n",
    "\n",
    "            print(f\"Extracting audio from: {file}\")\n",
    "\n",
    "            # FFmpeg command (CPU-based)\n",
    "            cmd = [\n",
    "                \"ffmpeg\",\n",
    "                \"-i\", input_path,\n",
    "                \"-vn\",             # Disable video\n",
    "                \"-acodec\", \"aac\",  # AAC encoder\n",
    "                \"-b:a\", \"192k\",    # Audio bitrate\n",
    "                output_path,\n",
    "                \"-y\"               # Overwrite output if exists\n",
    "            ]\n",
    "\n",
    "            # Run the command, suppressing FFmpeg output\n",
    "            subprocess.run(cmd, stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)\n",
    "\n",
    "    print(\"\\n✅ Audio extraction completed! Check your output folder.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 9033,
     "status": "ok",
     "timestamp": 1740960132393,
     "user": {
      "displayName": "NEXUS - DCG",
      "userId": "08238528941623053962"
     },
     "user_tz": 180
    },
    "id": "q0VcBMvSSzwj",
    "outputId": "90546686-800e-4ad7-fbf4-f247f22c4de8"
   },
   "outputs": [],
   "source": [
    "def convert_videos_to_audio(input_folder, output_folder):\n",
    "    \"\"\"\n",
    "    Convert video files in the supported formats to M4A audio using FFmpeg (CPU).\n",
    "    We keep it on CPU because GPU acceleration for audio extraction is typically\n",
    "    not a big performance boost, and can complicate dependencies.\n",
    "    \"\"\"\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "    supported_formats = (\".mp4\", \".mov\", \".avi\", \".mkv\")\n",
    "\n",
    "    for file in os.listdir(input_folder):\n",
    "        if file.lower().endswith(supported_formats):\n",
    "            input_path = os.path.join(input_folder, file)\n",
    "            output_name = os.path.splitext(file)[0] + \".m4a\"\n",
    "            output_path = os.path.join(output_folder, output_name)\n",
    "\n",
    "            print(f\"Extracting audio from: {file}\")\n",
    "\n",
    "            # FFmpeg command (CPU-based)\n",
    "            cmd = [\n",
    "                \"ffmpeg\",\n",
    "                \"-i\", input_path,\n",
    "                \"-vn\",             # Disable video\n",
    "                \"-acodec\", \"aac\",  # AAC encoder\n",
    "                \"-b:a\", \"192k\",    # Audio bitrate\n",
    "                output_path,\n",
    "                \"-y\"               # Overwrite output if exists\n",
    "            ]\n",
    "\n",
    "            # Run the command, suppressing FFmpeg output\n",
    "            subprocess.run(cmd, stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)\n",
    "\n",
    "    print(\"\\n✅ Audio extraction completed! Check your output folder.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 18088,
     "status": "ok",
     "timestamp": 1740960165290,
     "user": {
      "displayName": "NEXUS - DCG",
      "userId": "08238528941623053962"
     },
     "user_tz": 180
    },
    "id": "OP23zCgeTUp3",
    "outputId": "3e28035f-700a-486a-952a-2bab8db3a6cd"
   },
   "outputs": [],
   "source": [
    "def transcribe_audios(input_folder, output_folder, whisper_model=\"turbo\", language=None):\n",
    "    \"\"\"\n",
    "    Transcribe all .m4a audio files in the 'input_folder' using Whisper,\n",
    "    then save the transcripts as .txt files in the 'output_folder'.\n",
    "\n",
    "    Parameters:\n",
    "    - whisper_model (str): the model name to load (e.g., 'turbo', 'tiny', 'base', etc.)\n",
    "    - language (str or None): if you want to force a specific language (e.g. 'en', 'pt').\n",
    "    \"\"\"\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "    print(\"\\nLoading Whisper model:\", whisper_model)\n",
    "    model = whisper.load_model(whisper_model, device=device)\n",
    "    print(f\"✅ Whisper '{whisper_model}' model loaded successfully!\\n\")\n",
    "\n",
    "    for file_name in os.listdir(input_folder):\n",
    "        if file_name.lower().endswith(\".m4a\"):\n",
    "            input_path = os.path.join(input_folder, file_name)\n",
    "            output_text_path = os.path.join(\n",
    "                output_folder,\n",
    "                file_name.replace(\".m4a\", \".txt\")\n",
    "            )\n",
    "\n",
    "            print(f\"Transcribing: {file_name} ...\")\n",
    "\n",
    "            # Transcribe using GPU if available, otherwise CPU\n",
    "            if language:\n",
    "                result = model.transcribe(input_path, language=language)\n",
    "            else:\n",
    "                result = model.transcribe(input_path)\n",
    "\n",
    "            # Save transcript\n",
    "            with open(output_text_path, \"w\", encoding=\"utf-8\") as f:\n",
    "                f.write(result[\"text\"])\n",
    "\n",
    "            print(f\"Transcription saved to: {output_text_path}\")\n",
    "\n",
    "    print(\"\\n✅ All transcriptions completed successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 18815,
     "status": "ok",
     "timestamp": 1740960187874,
     "user": {
      "displayName": "NEXUS - DCG",
      "userId": "08238528941623053962"
     },
     "user_tz": 180
    },
    "id": "NHyha-FuTZL5",
    "outputId": "47086232-66a5-44be-ee80-d7914ba41053"
   },
   "outputs": [],
   "source": [
    "# 6) Example usage\n",
    "\n",
    "video_folder = \"/content/drive/My Drive/videos\"          # Your video files path\n",
    "audio_folder = \"/content/drive/My Drive/audios\"          # Where to store extracted audio\n",
    "transcript_folder = \"/content/drive/My Drive/transcripts\"  # Where to store transcriptions\n",
    "\n",
    "# Step A: Convert videos to audio (CPU for FFmpeg)\n",
    "convert_videos_to_audio(video_folder, audio_folder)\n",
    "\n",
    "# Step B: Transcribe using Whisper (GPU if available, otherwise CPU)\n",
    "transcribe_audios(\n",
    "    input_folder=audio_folder,\n",
    "    output_folder=transcript_folder,\n",
    "    whisper_model=\"turbo\",  # Keep \"turbo\" as requested \n",
    "    language=\"en\"           # Adjust to \"en\", \"pt\", or None for auto detection\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyPXzbR6N5nJ8+aZYpmhk6GO",
   "gpuType": "T4",
   "provenance": [
    {
     "file_id": "1Pq4tpThptKInswL_KsmeHU14xQWI4mzt",
     "timestamp": 1740960272099
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
